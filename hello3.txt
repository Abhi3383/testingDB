trigger:
- main  # Adjust to the branch you want to trigger the pipeline

pool:
  name: Azure Pipelines

stages:
- stage: PublishArtifacts
  displayName: 'Build Stage-Notebooks'
  jobs:
  - job: PublishJob
    displayName: 'Publish Notebooks Folder'
    steps:
    - task: CopyFiles@2
      displayName: 'Copy notebooks folder to Artifact Staging Directory'
      inputs:
        SourceFolder: '$(System.DefaultWorkingDirectory)'  # Root directory of the repo
        Contents: 'Notebook/**'  # Include only the notebooks folder and its contents
        TargetFolder: '$(Build.ArtifactStagingDirectory)'  # Destination directory 

    - task: PublishBuildArtifacts@1
      displayName: 'Publish Artifact: Legacy-notebook'
      inputs:
        PathtoPublish: '$(Build.ArtifactStagingDirectory)'
        ArtifactName: 'Legacy-notebook'




# Download Databricks CLI
Invoke-WebRequest -Uri "https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh" -OutFile "install.sh"
bash install.sh

# Create Databricks config file with necessary values
$databricksConfigPath = "$env:USERPROFILE\.databrickscfg"
@"
[DEFAULT]
host=
azure_tenant_id=
azure_client_id=
azure_client_secret=
jobs-api-version=2.1
"@ | Set-Content -Path $databricksConfigPath

# Navigate to the directory containing the artifact
Set-Location -Path "_databricksNotebook-CI/drop/Notebook"
Get-Location
Get-ChildItem -Force

# List workspaces in Databricks
databricks workspace list "//Workspace"

# List the directories inside the "notebook" folder (assuming they are folders to be exported)
$directories = Get-ChildItem -Directory
foreach ($dir in $directories) {
    Write-Host "Exporting directory: $($dir.Name)"
    # Import each directory into Databricks workspace
    databricks workspace import-dir "$($dir.FullName)" "//Workspace/$($dir.Name)" --overwrite
}

# Verify the import into the Databricks workspace
databricks workspace list "//Workspace"
