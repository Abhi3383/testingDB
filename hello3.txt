trigger:
- main  # Adjust to the branch you want to trigger the pipeline

pool:
  name: Azure Pipelines

stages:
- stage: PublishArtifacts
  displayName: 'Build Stage-Notebooks'
  jobs:
  - job: PublishJob
    displayName: 'Publish Notebooks Folder'
    steps:
    - task: CopyFiles@2
      displayName: 'Copy notebooks folder to Artifact Staging Directory'
      inputs:
        SourceFolder: '$(System.DefaultWorkingDirectory)'  # Root directory of the repo
        Contents: 'Notebook/**'  # Include only the notebooks folder and its contents
        TargetFolder: '$(Build.ArtifactStagingDirectory)'  # Destination directory 

    - task: PublishBuildArtifacts@1
      displayName: 'Publish Artifact: Legacy-notebook'
      inputs:
        PathtoPublish: '$(Build.ArtifactStagingDirectory)'
        ArtifactName: 'Legacy-notebook'




#!/bin/bash

# Download Databricks CLI
curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | bash
 
echo [DEFAULT] > ~/.databrickscfg
echo host= >> ~/.databrickscfg
echo azure_tenant_id= >> ~/.databrickscfg  # Correct Tenant ID
echo azure_client_id= >> ~/.databrickscfg  # Correct Client ID
echo azure_client_secret= >> ~/.databrickscfg
echo jobs-api-version=2.1 >> ~/.databrickscfg

# Navigate to the directory containing the artifact
cd "_databricksNotebook-CI/Legacy-notebook/Notebook"
pwd
ls -la

databricks workspace list //Workspace

# List the directories inside "notebook" folder (assuming they are folders to be exported)
for dir in */; do
  if [ -d "$dir" ]; then
    echo "Exporting directory: $dir"
    # Import each directory into Databricks workspace
    databricks workspace import-dir "$dir" "//Workspace/$(basename "$dir")" --overwrite
  fi
done

# Verify the import into the Databricks workspace
databricks workspace list "//Workspace"